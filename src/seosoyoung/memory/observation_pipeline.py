"""ê´€ì°° íŒŒì´í”„ë¼ì¸

ë§¤í„´ë§ˆë‹¤ Observerë¥¼ í˜¸ì¶œí•˜ì—¬ ì„¸ì…˜ ê´€ì°° ë¡œê·¸ë¥¼ ê°±ì‹ í•˜ê³ , ì¥ê¸° ê¸°ì–µ í›„ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

íë¦„:
1. pending ë²„í¼ ë¡œë“œ â†’ ì´ë²ˆ í„´ ë©”ì‹œì§€ì™€ í•©ì‚° â†’ ìµœì†Œ í† í° ë¯¸ë§Œì´ë©´ pendingì— ëˆ„ì  í›„ ìŠ¤í‚µ
2. Observer í˜¸ì¶œ (ë§¤í„´) â†’ ì„¸ì…˜ ê´€ì°° ë¡œê·¸ ê°±ì‹  â†’ pending ë¹„ìš°ê¸°
3. <candidates> íƒœê·¸ê°€ ìˆìœ¼ë©´ ì¥ê¸° ê¸°ì–µ í›„ë³´ ë²„í¼ì— ì ì¬
4. ê´€ì°° ë¡œê·¸ê°€ reflection ì„ê³„ì¹˜ë¥¼ ë„˜ìœ¼ë©´ Reflectorë¡œ ì••ì¶•
5. í›„ë³´ ë²„í¼ í† í° í•©ì‚° â†’ promotion ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ Promoter í˜¸ì¶œ
6. ì¥ê¸° ê¸°ì–µ í† í° â†’ compaction ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ Compactor í˜¸ì¶œ
"""

import logging
import re
from datetime import datetime, timezone
from typing import Optional

from seosoyoung.config import Config
from seosoyoung.memory.observer import Observer
from seosoyoung.memory.promoter import Compactor, Promoter
from seosoyoung.memory.reflector import Reflector
from seosoyoung.memory.store import MemoryRecord, MemoryStore
from seosoyoung.memory.token_counter import TokenCounter

logger = logging.getLogger(__name__)


def _send_debug_log(channel: str, text: str, thread_ts: str = "") -> str:
    """OM ë””ë²„ê·¸ ë¡œê·¸ë¥¼ ìŠ¬ë™ ì±„ë„ì— ë°œì†¡. ë©”ì‹œì§€ tsë¥¼ ë°˜í™˜.

    Args:
        channel: ë°œì†¡ ì±„ë„
        text: ë©”ì‹œì§€ í…ìŠ¤íŠ¸
        thread_ts: ìŠ¤ë ˆë“œ ì•µì»¤ ts (ìˆìœ¼ë©´ í•´ë‹¹ ìŠ¤ë ˆë“œì— ë‹µê¸€ë¡œ ë°œì†¡)
    """
    try:
        from seosoyoung.config import Config
        from slack_sdk import WebClient

        client = WebClient(token=Config.SLACK_BOT_TOKEN)
        kwargs: dict = {"channel": channel, "text": text}
        if thread_ts:
            kwargs["thread_ts"] = thread_ts
        resp = client.chat_postMessage(**kwargs)
        return resp["ts"]
    except Exception as e:
        logger.warning(f"OM ë””ë²„ê·¸ ë¡œê·¸ ë°œì†¡ ì‹¤íŒ¨: {e}")
        return ""


def _update_debug_log(channel: str, ts: str, text: str) -> None:
    """ê¸°ì¡´ ë””ë²„ê·¸ ë¡œê·¸ ë©”ì‹œì§€ë¥¼ ìˆ˜ì •"""
    if not ts:
        return
    try:
        from seosoyoung.config import Config
        from slack_sdk import WebClient

        client = WebClient(token=Config.SLACK_BOT_TOKEN)
        client.chat_update(channel=channel, ts=ts, text=text)
    except Exception as e:
        logger.warning(f"OM ë””ë²„ê·¸ ë¡œê·¸ ìˆ˜ì • ì‹¤íŒ¨: {e}")


def _format_tokens(n: int) -> str:
    """í† í° ìˆ˜ë¥¼ ì²œ ë‹¨ìœ„ ì½¤ë§ˆ í¬ë§·"""
    return f"{n:,}"


def _blockquote(text: str, max_chars: int = 800) -> str:
    """í…ìŠ¤íŠ¸ë¥¼ ìŠ¬ë™ blockquote í˜•ì‹ìœ¼ë¡œ ë³€í™˜. ê¸¸ë©´ ì˜ë¼ì„œ í‘œì‹œ."""
    if not text or not text.strip():
        return ""
    text = text.strip()
    if len(text) > max_chars:
        text = text[:max_chars] + "â€¦"
    lines = text.split("\n")
    return "\n".join(f">{line}" for line in lines)


def _extract_new_observations(
    existing: str | None, updated: str
) -> str:
    """ê¸°ì¡´ ê´€ì°°ê³¼ ê°±ì‹ ëœ ê´€ì°°ì„ ë¹„êµí•˜ì—¬ ìƒˆë¡œ ì¶”ê°€ëœ ì¤„ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Observerê°€ ì „ì²´ë¥¼ ì¬ì‘ì„±í•˜ë¯€ë¡œ, ê¸°ì¡´ ì¤„ ì§‘í•©ì— ì—†ëŠ” ì¤„ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not existing or not existing.strip():
        return updated

    existing_lines = set(line.strip() for line in existing.strip().splitlines() if line.strip())
    new_lines = []
    for line in updated.strip().splitlines():
        stripped = line.strip()
        if stripped and stripped not in existing_lines:
            new_lines.append(line)

    return "\n".join(new_lines) if new_lines else ""


def parse_candidate_entries(candidates_text: str) -> list[dict]:
    """<candidates> íƒœê·¸ ë‚´ìš©ì„ íŒŒì‹±í•˜ì—¬ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜.

    ê° ì¤„ì—ì„œ ì´ëª¨ì§€ ìš°ì„ ìˆœìœ„(ğŸ”´ğŸŸ¡ğŸŸ¢)ì™€ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if not candidates_text or not candidates_text.strip():
        return []

    entries = []
    now = datetime.now(timezone.utc).isoformat()

    for line in candidates_text.strip().split("\n"):
        line = line.strip()
        if not line:
            continue

        # ìš°ì„ ìˆœìœ„ ì´ëª¨ì§€ ì¶”ì¶œ
        priority = "ğŸŸ¢"  # ê¸°ë³¸ê°’
        for emoji in ("ğŸ”´", "ğŸŸ¡", "ğŸŸ¢"):
            if line.startswith(emoji):
                priority = emoji
                line = line[len(emoji):].strip()
                # "HIGH", "MEDIUM", "LOW" ì ‘ë‘ì‚¬ ì œê±°
                line = re.sub(r"^(HIGH|MEDIUM|LOW)\s*[-â€“â€”]?\s*", "", line).strip()
                break

        if line:
            entries.append({
                "ts": now,
                "priority": priority,
                "content": line,
            })

    return entries


async def observe_conversation(
    store: MemoryStore,
    observer: Observer,
    thread_ts: str,
    user_id: str,
    messages: list[dict],
    min_turn_tokens: int = 200,
    reflector: Optional[Reflector] = None,
    reflection_threshold: int = 20000,
    promoter: Optional[Promoter] = None,
    promotion_threshold: int = 5000,
    compactor: Optional[Compactor] = None,
    compaction_threshold: int = 15000,
    compaction_target: int = 8000,
    debug_channel: str = "",
    anchor_ts: str = "",
) -> bool:
    """ë§¤í„´ Observerë¥¼ í˜¸ì¶œí•˜ì—¬ ì„¸ì…˜ ê´€ì°° ë¡œê·¸ë¥¼ ê°±ì‹ í•˜ê³  í›„ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

    Args:
        store: ê´€ì°° ë¡œê·¸ ì €ì¥ì†Œ
        observer: Observer ì¸ìŠ¤í„´ìŠ¤
        thread_ts: ì„¸ì…˜(ìŠ¤ë ˆë“œ) íƒ€ì„ìŠ¤íƒ¬í”„ â€” ì €ì¥ í‚¤
        user_id: ì‚¬ìš©ì ID â€” ë©”íƒ€ë°ì´í„°ìš©
        messages: ì´ë²ˆ í„´ ëŒ€í™” ë‚´ì—­
        min_turn_tokens: ìµœì†Œ í„´ í† í° (ì´í•˜ ìŠ¤í‚µ)
        reflector: Reflector ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ ì••ì¶• ê±´ë„ˆëœ€)
        reflection_threshold: Reflector íŠ¸ë¦¬ê±° í† í° ì„ê³„ì¹˜
        promoter: Promoter ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ ìŠ¹ê²© ê±´ë„ˆëœ€)
        promotion_threshold: í›„ë³´ ë²„í¼ â†’ Promoter íŠ¸ë¦¬ê±° í† í° ì„ê³„ì¹˜
        compactor: Compactor ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ ì»´íŒ©ì…˜ ê±´ë„ˆëœ€)
        compaction_threshold: ì¥ê¸° ê¸°ì–µ â†’ Compactor íŠ¸ë¦¬ê±° í† í° ì„ê³„ì¹˜
        compaction_target: ì»´íŒ©ì…˜ ëª©í‘œ í† í°
        debug_channel: ë””ë²„ê·¸ ë¡œê·¸ë¥¼ ë°œì†¡í•  ìŠ¬ë™ ì±„ë„

    Returns:
        True: ê´€ì°° ìˆ˜í–‰ë¨, False: ìŠ¤í‚µ ë˜ëŠ” ì‹¤íŒ¨
    """
    sid = thread_ts
    log_label = f"session={thread_ts}"
    debug_ts = ""

    try:
        token_counter = TokenCounter()

        # 1. pending ë²„í¼ ë¡œë“œ â†’ ì´ë²ˆ í„´ ë©”ì‹œì§€ì™€ í•©ì‚°
        pending = store.load_pending_messages(thread_ts)
        if pending:
            messages = pending + messages

        turn_tokens = token_counter.count_messages(messages)

        # ìµœì†Œ í† í° ë¯¸ë‹¬ ì‹œ pending ë²„í¼ì— ëˆ„ì í•˜ê³  ìŠ¤í‚µ
        if turn_tokens < min_turn_tokens:
            # ì´ë²ˆ í„´ì˜ ìƒˆ ë©”ì‹œì§€ë¥¼ pendingì— ì¶”ê°€ (ê¸°ì¡´ pendingì€ íŒŒì¼ì— ì´ë¯¸ ìˆìŒ)
            new_messages = messages[len(pending):] if pending else messages
            if new_messages:
                store.append_pending_messages(thread_ts, new_messages)
            logger.info(
                f"ê´€ì°° ìŠ¤í‚µ ({log_label}): "
                f"{turn_tokens} tok < {min_turn_tokens} ìµœì†Œ"
            )
            if debug_channel:
                _send_debug_log(
                    debug_channel,
                    f":fast_forward: *OM ìŠ¤í‚µ* `{sid}`\n"
                    f">`ëˆ„ì  {_format_tokens(turn_tokens)} tok < {_format_tokens(min_turn_tokens)} ìµœì†Œ`",
                    thread_ts=anchor_ts,
                )
            return False

        # 2. ê¸°ì¡´ ê´€ì°° ë¡œê·¸ ë¡œë“œ
        record = store.get_record(thread_ts)
        existing_observations = record.observations if record else None

        # ë””ë²„ê·¸ ì´ë²¤íŠ¸ #1: ê´€ì°° ì‹œì‘ (send)
        if debug_channel:
            debug_ts = _send_debug_log(
                debug_channel,
                f":mag: *OM ê´€ì°° ì‹œì‘* `{sid}`",
                thread_ts=anchor_ts,
            )

        # 3. Observer í˜¸ì¶œ (ë§¤í„´)
        result = await observer.observe(
            existing_observations=existing_observations,
            messages=messages,
        )

        if result is None:
            logger.warning(f"Observerê°€ Noneì„ ë°˜í™˜ ({log_label})")
            if debug_channel:
                _update_debug_log(
                    debug_channel,
                    debug_ts,
                    f":x: *OM ê´€ì°° ì˜¤ë¥˜* `{sid}`\n>`Observer returned None`",
                )
            return False

        # 4. ê´€ì°° ë¡œê·¸ ê°±ì‹ 
        new_tokens = token_counter.count_string(result.observations)

        if record is None:
            record = MemoryRecord(thread_ts=thread_ts, user_id=user_id)

        record.observations = result.observations
        record.observation_tokens = new_tokens
        record.last_observed_at = datetime.now(timezone.utc)
        record.total_sessions_observed += 1

        # 5. í›„ë³´ ì ì¬
        candidate_count = 0
        candidate_summary = ""
        if result.candidates:
            entries = parse_candidate_entries(result.candidates)
            if entries:
                store.append_candidates(thread_ts, entries)
                candidate_count = len(entries)
                # ìš°ì„ ìˆœìœ„ë³„ ì¹´ìš´íŠ¸
                counts = {}
                for e in entries:
                    p = e["priority"]
                    counts[p] = counts.get(p, 0) + 1
                parts = []
                for emoji in ("ğŸ”´", "ğŸŸ¡", "ğŸŸ¢"):
                    if emoji in counts:
                        parts.append(f"{emoji}{counts[emoji]}")
                candidate_summary = " ".join(parts)

        # 6. Reflector: ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ ì••ì¶•
        if reflector and new_tokens > reflection_threshold:
            pre_tokens = new_tokens
            logger.info(
                f"Reflector íŠ¸ë¦¬ê±° ({log_label}): "
                f"{new_tokens} > {reflection_threshold} tokens"
            )
            reflection_result = await reflector.reflect(
                observations=record.observations,
                target_tokens=reflection_threshold // 2,
            )
            if reflection_result:
                record.observations = reflection_result.observations
                record.observation_tokens = reflection_result.token_count
                record.reflection_count += 1
                logger.info(
                    f"Reflector ì™„ë£Œ ({log_label}): "
                    f"{pre_tokens} â†’ {reflection_result.token_count} tokens"
                )
                # ë””ë²„ê·¸ ì´ë²¤íŠ¸ #2: Reflector (ë³„ë„ send)
                if debug_channel:
                    ref_quote = _blockquote(reflection_result.observations)
                    _send_debug_log(
                        debug_channel,
                        f":recycle: *OM ì„¸ì…˜ ê´€ì°° ì••ì¶•* `{sid}`\n"
                        f">`{_format_tokens(pre_tokens)} â†’ {_format_tokens(reflection_result.token_count)} tok`\n"
                        f"{ref_quote}",
                        thread_ts=anchor_ts,
                    )

        # 7. ìƒˆ ê´€ì°° diff ê³„ì‚° ë° ì €ì¥ + pending ë²„í¼ ë¹„ìš°ê¸°
        new_obs = _extract_new_observations(
            existing_observations, result.observations
        )
        store.save_new_observations(thread_ts, new_obs)
        store.save_record(record)
        store.clear_pending_messages(thread_ts)

        logger.info(
            f"ê´€ì°° ì™„ë£Œ ({log_label}): "
            f"{record.observation_tokens} tokens, "
            f"ì´ {record.total_sessions_observed}íšŒ"
            + (f", í›„ë³´ +{candidate_count}" if candidate_count else "")
        )

        # ë””ë²„ê·¸ ì´ë²¤íŠ¸ #1 ì™„ë£Œ (update) â€” ì´ë²¤íŠ¸ #3 (í›„ë³´ ì •ë³´) í†µí•©
        if debug_channel:
            if candidate_count:
                candidate_part = f" | í›„ë³´ +{candidate_count} ({candidate_summary})"
            else:
                candidate_part = " | í›„ë³´ ì—†ìŒ"
            new_obs_lines = len([l for l in new_obs.splitlines() if l.strip()]) if new_obs else 0
            new_obs_part = f" | ìƒˆ ê´€ì°° {new_obs_lines}ì¤„" if new_obs_lines else " | ìƒˆ ê´€ì°° ì—†ìŒ"
            _update_debug_log(
                debug_channel,
                debug_ts,
                f"{Config.EMOJI_TEXT_OBS_COMPLETE} *OM ê´€ì°° ì™„ë£Œ* `{sid}`\n"
                f">`{_format_tokens(turn_tokens)} tok{candidate_part}{new_obs_part}`",
            )

        # 8. Promoter: í›„ë³´ ë²„í¼ í† í° í•©ì‚° â†’ ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ ìŠ¹ê²©
        if promoter:
            await _try_promote(
                store=store,
                promoter=promoter,
                promotion_threshold=promotion_threshold,
                compactor=compactor,
                compaction_threshold=compaction_threshold,
                compaction_target=compaction_target,
                debug_channel=debug_channel,
                token_counter=token_counter,
                anchor_ts=anchor_ts,
            )

        return True

    except Exception as e:
        logger.error(f"ê´€ì°° íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜ ({log_label}): {e}")
        if debug_channel:
            error_msg = str(e)[:200]
            _update_debug_log(
                debug_channel,
                debug_ts,
                f":x: *OM ê´€ì°° ì˜¤ë¥˜* `{sid}`\n>`{error_msg}`",
            )
        return False


async def _try_promote(
    store: MemoryStore,
    promoter: Promoter,
    promotion_threshold: int,
    compactor: Optional[Compactor],
    compaction_threshold: int,
    compaction_target: int,
    debug_channel: str,
    token_counter: TokenCounter,
    anchor_ts: str = "",
) -> None:
    """í›„ë³´ ë²„í¼ í† í°ì´ ì„ê³„ì¹˜ë¥¼ ë„˜ìœ¼ë©´ Promoterë¥¼ í˜¸ì¶œí•˜ê³ , í•„ìš” ì‹œ Compactorë„ í˜¸ì¶œ."""
    try:
        candidate_tokens = store.count_all_candidate_tokens()
        if candidate_tokens < promotion_threshold:
            return

        all_candidates = store.load_all_candidates()
        if not all_candidates:
            return

        # ê¸°ì¡´ ì¥ê¸° ê¸°ì–µ ë¡œë“œ
        persistent_data = store.get_persistent()
        existing_persistent = persistent_data["content"] if persistent_data else ""

        # ë””ë²„ê·¸ ì´ë²¤íŠ¸ #4: Promoter ì‹œì‘ (send)
        promoter_debug_ts = ""
        if debug_channel:
            promoter_debug_ts = _send_debug_log(
                debug_channel,
                f":brain: *LTM ìŠ¹ê²© ê²€í†  ì‹œì‘*\n"
                f">`í›„ë³´ {_format_tokens(candidate_tokens)} tok ({len(all_candidates)}ê±´)`",
                thread_ts=anchor_ts,
            )

        logger.info(
            f"Promoter íŠ¸ë¦¬ê±°: {candidate_tokens} tok ({len(all_candidates)}ê±´)"
        )

        result = await promoter.promote(
            candidates=all_candidates,
            existing_persistent=existing_persistent,
        )

        # ìŠ¹ê²©ëœ í•­ëª©ì´ ìˆìœ¼ë©´ ì¥ê¸° ê¸°ì–µì— ë¨¸ì§€
        if result.promoted and result.promoted.strip():
            merged = Promoter.merge_promoted(existing_persistent, result.promoted)
            persistent_tokens = token_counter.count_string(merged)

            store.save_persistent(
                content=merged,
                meta={
                    "token_count": persistent_tokens,
                    "last_promoted_at": datetime.now(timezone.utc).isoformat(),
                },
            )

            logger.info(
                f"Promoter ì™„ë£Œ: ìŠ¹ê²© {result.promoted_count}ê±´, "
                f"ê¸°ê° {result.rejected_count}ê±´, "
                f"ì¥ê¸°ê¸°ì–µ {persistent_tokens} tok"
            )

            # ë””ë²„ê·¸ ì´ë²¤íŠ¸ #5: Promoter ì™„ë£Œ â€” ìŠ¹ê²© ìˆìŒ (update #4)
            if debug_channel:
                priority_parts = []
                for emoji in ("ğŸ”´", "ğŸŸ¡", "ğŸŸ¢"):
                    cnt = result.priority_counts.get(emoji, 0)
                    if cnt:
                        priority_parts.append(f"{emoji}{cnt}")
                priority_str = " ".join(priority_parts)
                promoted_quote = _blockquote(result.promoted)
                _update_debug_log(
                    debug_channel,
                    promoter_debug_ts,
                    f"{Config.EMOJI_TEXT_OBS_COMPLETE} *LTM ìŠ¹ê²© ì™„ë£Œ*\n"
                    f">`ìŠ¹ê²© {result.promoted_count}ê±´ ({priority_str}) | "
                    f"ê¸°ê° {result.rejected_count}ê±´ | "
                    f"ì¥ê¸°ê¸°ì–µ {_format_tokens(persistent_tokens)} tok`\n"
                    f"{promoted_quote}",
                )

            # Compactor íŠ¸ë¦¬ê±° ì²´í¬
            if compactor and persistent_tokens > compaction_threshold:
                await _try_compact(
                    store=store,
                    compactor=compactor,
                    compaction_target=compaction_target,
                    persistent_tokens=persistent_tokens,
                    debug_channel=debug_channel,
                    anchor_ts=anchor_ts,
                )
        else:
            logger.info(
                f"Promoter ì™„ë£Œ: ìŠ¹ê²© 0ê±´, ê¸°ê° {result.rejected_count}ê±´"
            )

            # ë””ë²„ê·¸ ì´ë²¤íŠ¸ #5: ìŠ¹ê²© ì—†ìŒ (update #4)
            if debug_channel:
                _update_debug_log(
                    debug_channel,
                    promoter_debug_ts,
                    f"{Config.EMOJI_TEXT_OBS_COMPLETE} *LTM ìŠ¹ê²© ì™„ë£Œ*\n"
                    f">`ìŠ¹ê²© 0ê±´ | ê¸°ê° {result.rejected_count}ê±´`",
                )

        # í›„ë³´ ë²„í¼ ë¹„ìš°ê¸°
        store.clear_all_candidates()

    except Exception as e:
        logger.error(f"Promoter íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")


async def _try_compact(
    store: MemoryStore,
    compactor: Compactor,
    compaction_target: int,
    persistent_tokens: int,
    debug_channel: str,
    anchor_ts: str = "",
) -> None:
    """ì¥ê¸° ê¸°ì–µ í† í°ì´ ì„ê³„ì¹˜ë¥¼ ë„˜ìœ¼ë©´ archive í›„ Compactorë¥¼ í˜¸ì¶œ."""
    try:
        # archive ë°±ì—…
        archive_path = store.archive_persistent()
        logger.info(
            f"Compactor íŠ¸ë¦¬ê±°: {persistent_tokens} tok, archive={archive_path}"
        )

        # ì¥ê¸° ê¸°ì–µ ë¡œë“œ
        persistent_data = store.get_persistent()
        if not persistent_data:
            return

        result = await compactor.compact(
            persistent=persistent_data["content"],
            target_tokens=compaction_target,
        )

        # ì••ì¶• ê²°ê³¼ ì €ì¥
        store.save_persistent(
            content=result.compacted,
            meta={
                "token_count": result.token_count,
                "last_compacted_at": datetime.now(timezone.utc).isoformat(),
            },
        )

        logger.info(
            f"Compactor ì™„ë£Œ: {persistent_tokens} â†’ {result.token_count} tok"
        )

        # ë””ë²„ê·¸ ì´ë²¤íŠ¸ #6: ì»´íŒ©ì…˜ (ë³„ë„ send)
        if debug_channel:
            compact_quote = _blockquote(result.compacted)
            archive_info = f"\n>`archive: {archive_path}`" if archive_path else ""
            _send_debug_log(
                debug_channel,
                f":compression: *LTM ì¥ê¸° ê¸°ì–µ ì••ì¶•*\n"
                f">`{_format_tokens(persistent_tokens)} â†’ {_format_tokens(result.token_count)} tok`"
                f"{archive_info}\n"
                f"{compact_quote}",
                thread_ts=anchor_ts,
            )

    except Exception as e:
        logger.error(f"Compactor íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
