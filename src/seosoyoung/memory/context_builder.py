"""ì»¨í…ìŠ¤íŠ¸ ë¹Œë”

ì¥ê¸° ê¸°ì–µê³¼ ì„¸ì…˜ ê´€ì°° ë¡œê·¸ë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ Claude ì„¸ì…˜ì— ì£¼ì…í•©ë‹ˆë‹¤.
OMì˜ processInputStepì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.

ì£¼ì… ê³„ì¸µ:
- ì¥ê¸° ê¸°ì–µ (persistent/recent.json): ë§¤ ì„¸ì…˜ ì‹œì‘ ì‹œ í•­ìƒ ì£¼ì…
- ì„¸ì…˜ ê´€ì°° (observations/{thread_ts}.json): inject í”Œë˜ê·¸ ìˆì„ ë•Œë§Œ ì£¼ì…
- ì±„ë„ ê´€ì°° (channel/{channel_id}/): ê´€ì°° ëŒ€ìƒ ì±„ë„ì—ì„œ ë©˜ì…˜ë  ë•Œ ì£¼ì…
"""

from __future__ import annotations

import json
import logging
import re
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import TYPE_CHECKING, Optional

from seosoyoung.memory.store import MemoryStore
from seosoyoung.memory.token_counter import TokenCounter

if TYPE_CHECKING:
    from seosoyoung.memory.channel_store import ChannelStore

logger = logging.getLogger(__name__)


@dataclass
class InjectionResult:
    """ì£¼ì… ê²°ê³¼ â€” ë””ë²„ê·¸ ë¡œê·¸ìš© ì •ë³´ë¥¼ í¬í•¨"""

    prompt: str | None
    persistent_tokens: int = 0
    session_tokens: int = 0
    persistent_content: str = ""
    session_content: str = ""
    channel_digest_tokens: int = 0
    channel_buffer_tokens: int = 0
    new_observation_tokens: int = 0
    new_observation_content: str = ""


# â”€â”€ í•­ëª© ë Œë”ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def render_observation_items(items: list[dict], now: datetime | None = None) -> str:
    """ê´€ì°° í•­ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë Œë”ë§í•©ë‹ˆë‹¤."""
    if not items:
        return ""

    if now is None:
        now = datetime.now(timezone.utc)
    if now.tzinfo is None:
        now = now.replace(tzinfo=timezone.utc)

    lines: list[str] = []
    current_date = None

    for item in items:
        session_date = item.get("session_date", "")
        if session_date != current_date:
            current_date = session_date
            relative = _relative_time_str(session_date, now) if session_date else ""
            if lines:
                lines.append("")  # ì„¹ì…˜ ì‚¬ì´ ë¹ˆ ì¤„
            if relative:
                lines.append(f"## [{session_date}] ({relative})")
            elif session_date:
                lines.append(f"## [{session_date}]")
            lines.append("")

        priority = item.get("priority", "ğŸŸ¢")
        content = item.get("content", "")
        lines.append(f"{priority} {content}")

    return "\n".join(lines)


def render_persistent_items(items: list[dict]) -> str:
    """ì¥ê¸° ê¸°ì–µ í•­ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë Œë”ë§í•©ë‹ˆë‹¤."""
    if not items:
        return ""
    lines = []
    for item in items:
        priority = item.get("priority", "ğŸŸ¢")
        content = item.get("content", "")
        lines.append(f"{priority} {content}")
    return "\n".join(lines)


def _relative_time_str(date_str: str, now: datetime) -> str:
    """ë‚ ì§œ ë¬¸ìì—´ì— ëŒ€í•œ ìƒëŒ€ ì‹œê°„ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        obs_date = datetime.strptime(date_str, "%Y-%m-%d").replace(
            tzinfo=timezone.utc
        )
        delta = now - obs_date
        days = delta.days

        if days == 0:
            return "ì˜¤ëŠ˜"
        elif days == 1:
            return "ì–´ì œ"
        elif days < 7:
            return f"{days}ì¼ ì „"
        elif days < 30:
            return f"{days // 7}ì£¼ ì „"
        elif days < 365:
            return f"{days // 30}ê°œì›” ì „"
        else:
            return f"{days // 365}ë…„ ì „"
    except ValueError:
        return ""


# â”€â”€ í•­ëª© ìµœì í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def optimize_items_for_context(
    items: list[dict], max_tokens: int = 30000
) -> list[dict]:
    """ê´€ì°° í•­ëª©ì„ ì»¨í…ìŠ¤íŠ¸ ì£¼ì…ì— ìµœì í™”í•©ë‹ˆë‹¤.

    í† í° ìˆ˜ ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ ë‚®ì€ ìš°ì„ ìˆœìœ„ í•­ëª©ë¶€í„° ì œê±°í•©ë‹ˆë‹¤.
    """
    counter = TokenCounter()
    rendered = render_observation_items(items)
    token_count = counter.count_string(rendered)

    if token_count <= max_tokens:
        return items

    # ìš°ì„ ìˆœìœ„ ê°€ì¤‘ì¹˜ (ë‚®ì„ìˆ˜ë¡ ë¨¼ì € ì œê±°)
    priority_weight = {"ğŸŸ¢": 0, "ğŸŸ¡": 1, "ğŸ”´": 2}

    # ì œê±° ìˆœì„œ: ë‚®ì€ ìš°ì„ ìˆœìœ„ + ì˜¤ë˜ëœ ê²ƒë¶€í„°
    sorted_items = sorted(
        enumerate(items),
        key=lambda x: (
            priority_weight.get(x[1].get("priority", "ğŸŸ¢"), 0),
            x[1].get("session_date", ""),
        ),
    )

    remove_indices: set[int] = set()
    for idx, _item in sorted_items:
        remove_indices.add(idx)
        remaining = [it for i, it in enumerate(items) if i not in remove_indices]
        rendered = render_observation_items(remaining)
        if counter.count_string(rendered) <= max_tokens:
            return remaining

    return []


# â”€â”€ í•˜ìœ„ í˜¸í™˜ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def add_relative_time(observations: str, now: datetime | None = None) -> str:
    """[í•˜ìœ„ í˜¸í™˜] í…ìŠ¤íŠ¸ ê´€ì°° ë¡œê·¸ì˜ ë‚ ì§œ í—¤ë”ì— ìƒëŒ€ ì‹œê°„ ì£¼ì„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

    ## [2026-02-10] â†’ ## [2026-02-10] (3ì¼ ì „)
    """
    if now is None:
        now = datetime.now(timezone.utc)
    if now.tzinfo is None:
        now = now.replace(tzinfo=timezone.utc)

    def replace_date_header(match: re.Match) -> str:
        date_str = match.group(1)
        relative = _relative_time_str(date_str, now)
        if relative:
            return f"## [{date_str}] ({relative})"
        return match.group(0)

    return re.sub(r"## \[(\d{4}-\d{2}-\d{2})\]", replace_date_header, observations)


def optimize_for_context(
    observations: str, max_tokens: int = 30000
) -> str:
    """[í•˜ìœ„ í˜¸í™˜] í…ìŠ¤íŠ¸ ê´€ì°° ë¡œê·¸ë¥¼ ì»¨í…ìŠ¤íŠ¸ ì£¼ì…ì— ìµœì í™”í•©ë‹ˆë‹¤."""
    counter = TokenCounter()
    token_count = counter.count_string(observations)

    if token_count <= max_tokens:
        return observations

    # ì„¹ì…˜ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (## [ë‚ ì§œ] ê¸°ì¤€)
    sections = re.split(r"(?=^## \[)", observations, flags=re.MULTILINE)
    sections = [s for s in sections if s.strip()]

    # ìµœì‹  ì„¹ì…˜ë¶€í„° ì—­ìˆœìœ¼ë¡œ ì¶”ê°€
    result_sections = []
    current_tokens = 0
    for section in reversed(sections):
        section_tokens = counter.count_string(section)
        if current_tokens + section_tokens > max_tokens:
            break
        result_sections.insert(0, section)
        current_tokens += section_tokens

    if not result_sections:
        low, high = 0, len(observations)
        while low < high:
            mid = (low + high + 1) // 2
            if counter.count_string(observations[-mid:]) <= max_tokens:
                low = mid
            else:
                high = mid - 1
        return observations[-low:] if low > 0 else observations[:1000]

    return "".join(result_sections)


# â”€â”€ ì»¨í…ìŠ¤íŠ¸ ë¹Œë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class ContextBuilder:
    """ì¥ê¸° ê¸°ì–µ + ì„¸ì…˜ ê´€ì°° ë¡œê·¸ + ì±„ë„ ê´€ì°°ì„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""

    def __init__(
        self,
        store: MemoryStore,
        channel_store: Optional["ChannelStore"] = None,
    ):
        self.store = store
        self.channel_store = channel_store
        self._counter = TokenCounter()

    def _build_channel_observation(
        self,
        channel_id: str,
        thread_ts: Optional[str] = None,
    ) -> tuple[str, int, int]:
        """ì±„ë„ ê´€ì°° ì»¨í…ìŠ¤íŠ¸ë¥¼ XML ë¬¸ìì—´ë¡œ ë¹Œë“œí•©ë‹ˆë‹¤.

        Returns:
            (xml_string, digest_tokens, buffer_tokens)
        """
        if not self.channel_store or not channel_id:
            return "", 0, 0

        digest_tokens = 0
        buffer_tokens = 0
        sections = []

        # digest
        digest_data = self.channel_store.get_digest(channel_id)
        if digest_data and digest_data["content"].strip():
            digest_content = digest_data["content"]
            digest_tokens = self._counter.count_string(digest_content)
            sections.append(f"<digest>\n{digest_content}\n</digest>")

        # channel buffer (ë¯¸ì†Œí™” ì±„ë„ ë£¨íŠ¸ ë©”ì‹œì§€)
        channel_messages = self.channel_store.load_channel_buffer(channel_id)
        if channel_messages:
            lines = [json.dumps(m, ensure_ascii=False) for m in channel_messages]
            buf_text = "\n".join(lines)
            buffer_tokens += self._counter.count_string(buf_text)
            sections.append(f"<recent-channel>\n{buf_text}\n</recent-channel>")

        # thread buffer (í˜„ì¬ ìŠ¤ë ˆë“œë§Œ)
        if thread_ts:
            thread_messages = self.channel_store.load_thread_buffer(
                channel_id, thread_ts
            )
            if thread_messages:
                lines = [json.dumps(m, ensure_ascii=False) for m in thread_messages]
                buf_text = "\n".join(lines)
                buffer_tokens += self._counter.count_string(buf_text)
                sections.append(
                    f'<recent-thread thread="{thread_ts}">\n{buf_text}\n</recent-thread>'
                )

        if not sections:
            return "", 0, 0

        inner = "\n\n".join(sections)
        xml = f'<channel-observation channel="{channel_id}">\n{inner}\n</channel-observation>'
        return xml, digest_tokens, buffer_tokens

    def build_memory_prompt(
        self,
        thread_ts: str,
        max_tokens: int = 30000,
        include_persistent: bool = False,
        include_session: bool = True,
        include_channel_observation: bool = False,
        channel_id: Optional[str] = None,
        include_new_observations: bool = False,
    ) -> InjectionResult:
        """ì¥ê¸° ê¸°ì–µ, ì„¸ì…˜ ê´€ì°°, ì±„ë„ ê´€ì°°, ìƒˆ ê´€ì°°ì„ í•©ì³ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        ì£¼ì… ìˆœì„œ: ì¥ê¸° ê¸°ì–µ â†’ ìƒˆ ê´€ì°° â†’ ì„¸ì…˜ ê´€ì°° â†’ ì±„ë„ ê´€ì°°
        """
        parts = []
        persistent_tokens = 0
        session_tokens = 0
        persistent_content = ""
        session_content = ""
        channel_digest_tokens = 0
        channel_buffer_tokens = 0
        new_observation_tokens = 0
        new_observation_content = ""

        # 1. ì¥ê¸° ê¸°ì–µ (persistent/recent.json)
        if include_persistent:
            persistent_data = self.store.get_persistent()
            if persistent_data and persistent_data["content"]:
                items = persistent_data["content"]
                content = render_persistent_items(items)
                if content.strip():
                    persistent_tokens = self._counter.count_string(content)
                    persistent_content = content
                    parts.append(
                        "<long-term-memory>\n"
                        "ë‹¤ìŒì€ ê³¼ê±° ëŒ€í™”ë“¤ì—ì„œ ì¶•ì í•œ ì¥ê¸° ê¸°ì–µì…ë‹ˆë‹¤.\n"
                        "ì‘ë‹µí•  ë•Œ ì´ ê¸°ì–µì„ ìì—°ìŠ¤ëŸ½ê²Œ í™œìš©í•˜ì„¸ìš”.\n\n"
                        f"{content}\n"
                        "</long-term-memory>"
                    )

        # 2. ìƒˆ ê´€ì°° (í˜„ì¬ ì„¸ì…˜ì˜ ì´ì „ í„´ì—ì„œ ìƒˆë¡œ ì¶”ê°€ëœ ê´€ì°° diff)
        if include_new_observations:
            new_obs_items = self.store.get_new_observations(thread_ts)
            if new_obs_items:
                observations_text = render_observation_items(new_obs_items)
                if observations_text.strip():
                    new_observation_tokens = self._counter.count_string(
                        observations_text
                    )
                    new_observation_content = observations_text
                    parts.append(
                        "<new-observations>\n"
                        "ì´ì „ í„´ì˜ ëŒ€í™”ì—ì„œ ìƒˆë¡­ê²Œ ê´€ì°°ëœ ì‚¬ì‹¤ì…ë‹ˆë‹¤.\n\n"
                        f"{observations_text}\n"
                        "</new-observations>"
                    )
                    self.store.clear_new_observations(thread_ts)

        # 3. ì„¸ì…˜ ê´€ì°° (observations/{thread_ts}.json)
        if include_session:
            record = self.store.get_record(thread_ts)
            if record and record.observations:
                optimized_items = optimize_items_for_context(
                    record.observations, max_tokens
                )
                observations_text = render_observation_items(optimized_items)
                if observations_text.strip():
                    session_tokens = self._counter.count_string(observations_text)
                    session_content = observations_text
                    parts.append(
                        "<observational-memory>\n"
                        "ë‹¤ìŒì€ ì´ ì„¸ì…˜ì˜ ìµœê·¼ ëŒ€í™”ì—ì„œ ê´€ì°°í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\n\n"
                        f"{observations_text}\n"
                        "</observational-memory>"
                    )

        # 4. ì±„ë„ ê´€ì°° (channel/{channel_id}/)
        if include_channel_observation and channel_id:
            ch_xml, ch_digest_tok, ch_buf_tok = self._build_channel_observation(
                channel_id,
                thread_ts=thread_ts,
            )
            if ch_xml:
                channel_digest_tokens = ch_digest_tok
                channel_buffer_tokens = ch_buf_tok
                parts.append(ch_xml)

        prompt = "\n\n".join(parts) if parts else None

        return InjectionResult(
            prompt=prompt,
            persistent_tokens=persistent_tokens,
            session_tokens=session_tokens,
            persistent_content=persistent_content,
            session_content=session_content,
            channel_digest_tokens=channel_digest_tokens,
            channel_buffer_tokens=channel_buffer_tokens,
            new_observation_tokens=new_observation_tokens,
            new_observation_content=new_observation_content,
        )
